---
title: "DATA100 Group Project -- 2022 Fall"
author: "Shengda Hu"
date: "04/11/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# repos="https://utstat.toronto.edu/cran/"
library(tidyverse)
library(lubridate)
library(readxl)
library(maps)
```

# R Markdown information

Chapter 21 - 24 of the textbook, or online [27 - 30](https://r4ds.had.co.nz/communicate-intro.html)

Especially useful are the code block options on **Page 430 -- 431** (online [27 R Markdown, section 4](https://r4ds.had.co.nz/r-markdown.html))

# Project introduction

In the recent years, natural disasters are more and more prominently in the news stories, despite (or because) of the constant human struggles. Many regard them as symptoms of climate change due to human actions, while some think they are just climate doing what it has been doing -- variations, and still many focus on if the population is (un)able to adjust to it. Many believes that humans are exacerbating the changes of climate, while others feel there is nothing that cannot be explained by coincidences of natural factors.

This project will ask you to try and connect some dots in this topic. The main goal of this project is to go through the exploratory data analysis, and some modelling to understand as much as possible what the stories the data sets available might tell with the tools available.

The theme of the story that we would like to understand is the following: 

**How does human action / opinion / feelings relate to climate**. 

We are not seeking to decipher the cause and effect on this issue. *Relation* is a very loose notion, which of course have the cause-effect as a special case. Things could be related but not causing each other.

The data are mostly from online sources, which may not be tidy as is. Part of the job is to clean-up the data so that they becomes tidy, before proceed to plot, join, and model them, using techniques we have studied in this course. The report would contain the interpretation of the result you obtain and try to build a coherent story based on the data analysis that you will carry out.

It is also completely reasonable that, from the data sets we have, it may appear that some factors do not correlate much to the climate -- which is also knowledge gained.

In the following, we provide some data sets concerning a number of potential factors of interest, such as *sea ice*, *hurricane*, *energy use*, *opinions on climate*, and *COVID* data. We included also two sets of data concerning United States, in terms of the distribution of population affected by drought as well as the so-called social capital. They can be roughly classified into *climate*, *social* and *other* factors. As you will see, a number of them are not up-to-date, which is due to the availability of timely data -- most of the interesting current data are not open data, or not easy to locate in more readily useful form to us. Most of these data are from online sources, and you are encouraged to track the most up-to-date version. We included the *WorldRegions.csv* data from World Regions Classification list on Wikipedia.

Also included are the data *WorldHappinessReport2022-Score.csv* from the World Happiness Report `2022`, which concerns the years `2018-2020`. It is computed based on the answers of people to the following question: “Please imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?” ([Statistical Appendix 1 for Chapter 2](https://happiness-report.s3.amazonaws.com/2022/Appendix_1_StatiscalAppendix_Ch2.pdf) of [World Health Report 2022](https://worldhappiness.report/ed/2022/)) Thus, the score can be seen as giving one interpretation of happiness.

## Using Map

A map can be a useful way of presenting data with geographical information. As an example, the map below shows the `new cases` on Nov. 01, 2022 obtained from [Our world in data](https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv). 

```{r include=FALSE}
COVID_cases <- read_csv("COVID-2022-11-01.csv",
                        col_types = cols(
                          location = col_character(),
                          new_cases = col_double(),
                          total_cases = col_double()
                        ))
```

```{r echo=FALSE, warning=FALSE}
world <- map_data("world")

iu <- COVID_cases %>% rename (region = location)

 # to match world map data, see comments below
iu$region[42] <- "Republic of Congo"
iu$region[44] <- "Ivory Coast"
iu$region[48] <- "Czech Republic"
iu$region[49] <- "Democratic Republic of the Congo"
iu$region[64] <- "Faroe Islands"
iu$region[128] <- "Micronesia"
iu$region[194] <- "Timor"
iu$region[203] <- "UK"
iu$region[204] <- "USA"

iu <- semi_join(iu, world, by = "region") #only keep countries according to world map data

# code below is modified from 
# https://stackoverflow.com/questions/29614972/ggplot-us-state-map-colors-are-fine-polygons-jagged-r
gg <- ggplot()

gg <- gg + geom_map(
  data = world,
  map = world,
  aes(x = long, y = lat, map_id = region),
  fill = "#ffffff",
  color = "#ffffff",
  size = 0.20
  )
  
  gg <- gg + geom_map(
  data = iu,
  map = world,
  aes(fill = new_cases, map_id = region),
  color = "#ffffff",
  size = 0.15
  )
  
  gg <- gg + scale_fill_continuous(low = 'thistle2', high = 'darkblue',
  guide = 'colorbar')
  gg
```

Sometimes, different data sets use different names for the same region / country. You may run the code block below (and remove the `include=FALSE` option) to see the mismatch between some names of the (same) regions in the two data sets `world` and `COVID_cases`. It means that the `semi_join` performed above showing the map is not exactly perfect. Adaptation to `iu$region` was made in the plot above, while there are still some that are not dealt with, e.g. `Antigua and Barbuda` is one row in `COVID_cases`, while two rows are used in `world` map data. Similar issues might affect other data sets provided and you may need to manually change some of them so that they have the same names throughout.

```{r include=FALSE}
world %>% distinct(region) %>% anti_join(COVID_cases, by = c("region" = "location"))
COVID_cases %>% distinct(location) %>% anti_join(world, by = c("location" = "region"))
```

# The Setup:

All data are gleaned from online sources. Some are in the form of Excel spreadsheets, which need to be read using `read_excel`. Examples are given in the code below on how to get data from a particular sheet. **Note** For Excel data, you should include code to load the spreadsheets and save the sheets that you intend to use in your analysis as separate `.csv` files in the beginning of your RMD file.

There are a total of **9** data collections as described below, **2** of which are online real-time data sets that are regularly updated, while the remaining **7** can be obtained as `csv` files on MyLS. You may need to make the data tidy for some of them. Please note that the data sets are from different sources, you may need to first make sure, for example, the country / region names provided indeed do correspond.

Aside from `regionclassification`, you must use data from at least `4` collections provided below. Provide brief description on the choice of data sets included in your project. Each collection contains more than one sets of data and different groups may work on different data even when choosing the same collections. You are encouraged to discuss among groups, while each group should work their own analysis and write their own report.

You can look at other data sets that are not provided below for inspirations and ideas. You *should not* include data from other sources in your report.

# The Questions:

- Based on the interpretation of your group, analyze how the current / changing climate situation relate to the various factors, or among themselves. The relationship can be in terms of potential causes, potential effects, or simply correlations. Make your arguments as clear as possible with the help of the data you choose to analyze. If two factors are shown to be not really related, that is also knowledge.

- Find *one* online article discussing how climate affects / is affected by communities / populations. Discuss if any analysis you have done in the project can be used to *counter* or *strengthen* the main arguments presented in the article.

# The Data sets

The sources of the data are contained in the hyperlink. They are the following:

- Our World in Data: from which we obtain the online data sets on COVID-19 cases, vaccinations and testing
- Wikipedia: from which we obtained the World Regions Classification data set, aside from the democracy index mentioned above
- Meta (formerly Facebook): from which we obtained the data on climate opinions as well as U.S. social capital distribution.
- Various U.S. agencies / collaborations: from which the rest of climate data are obtained.

#### Datasets contained in project files, as `.csv` or `.xlsx`

- `regionclassification`: [World Regions Classification](https://meta.wikimedia.org/wiki/List_of_countries_by_regional_classification)
```{r include=FALSE}
regionclassification <- read_tsv("WorldRegions.csv")
regionclassification %>% head()
```

- `happinessscore`: [World happiness report 2022, happiness score](https://worldhappiness.report/ed/2022/)
```{r include=FALSE}
happinessscore <- read_tsv("WorldHappinessReport2022-Score.csv")
happinessscore %>% head()
```

- Most recent cyclone data from [NOAA](https://www.nhc.noaa.gov/data/hurdat). There are two data sets:
  + [Atlantic](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2021-100522.txt)
  + [Pacific](https://www.nhc.noaa.gov/data/hurdat/hurdat2-nepac-1949-2021-091522.txt)

The updated data descriptions for them are respectively at

  + [Atlantic](https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atl-1851-2021.pdf)
  + [Pacific](https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-nencpac-1949-2021.pdf)
  
We also discussed cleaning them up and making them more useful in the lectures (`Lecture-08-01.Rmd`).

```{r include=FALSE}
# code copied from Lecture-08-01.Rmd
cyclone_data_address <- "https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2021-100522.txt"
cyclone_2021_raw <- 
  read_csv(cyclone_data_address,
           col_names = c(as.character(1:4))) %>%
  separate(`4`, into = c(as.character(4:21)), sep = ", ") %>%
  mutate(across(everything(), ~na_if(.,"-999"))) %>%
  mutate(across(everything(), ~na_if(.,"-99")))

cyclone_2021_raw %>% head()
```

- `Sea ice index` data from [National Snow and Ice Datacenter](https://nsidc.org/data/seaice_index/data-and-image-archive). They include two spreadsheet files:

  + `Sea_Ice_Index_Monthly_Data_by_Year_G02135_v3.0.xlsx`
  + `Sea_Ice_Index_Rates_of_Change_G02135_v3.0.xlsx`

You should be able to load the individual sheets in the files using the format below. In particular, the sheets in the file `Sea_Ice_Index_Monthly_Data_by_Year_G02135_v3.0.xlsx` are:

    + "NH-Extent", 
    + "NH-Area", 
    + "SH-Extent", 
    + "SH-Area"
    
The sheets in the file `Sea_Ice_Index_Rates_of_Change_G02135_v3.0.xlsx` are:

    + "NH-Ice-Change-Mkm^2-per-Month",
    + "NH-Ice-Change-km^2-per-Day",
    + "NH-Ice-Change-mi^2-per-Month",
    + "NH-Ice-Change-mi^2-per-Day",
    + "SH-Ice-Change-Mkm^2-per-Month",
    + "SH-Ice-Change-km^2-per-Day", 
    + "SH-Ice-Change-mi^2-per-Month",
    + "SH-Ice-Change-mi^2-per-Day"

Furthermore, the file

    "Sea_Ice_documentation.xlsx"

contains the description / meta-data for the data sheets above.

**Note**: It might be necessary to skip the first (few) rows in a sheet, just like what could happen with reading `.csv` files correctly using `read_csv`. Also, the resulting dataframes are mostly *not tidy*. You may need to tidy them first.

```{r include=FALSE}
Sea_Ice_Data_file <- "Sea_Ice_Index_Monthly_Data_by_Year_G02135_v3.0.xlsx"
Sea_Ice_Change_file <- "Sea_Ice_Index_Rates_of_Change_G02135_v3.0.xlsx"

NH_Extent <- read_excel(Sea_Ice_Data_file, sheet = "NH-Extent")
NH_Change_by_day <- read_excel(Sea_Ice_Change_file, sheet = "NH-Ice-Change-km^2-per-Day", skip = 1)

NH_Extent %>% head()
NH_Change_by_day %>% head()
```

- `climate_opinions` data from Meta (formerly Facebook), hosted on [HDX humdata.org](https://data.humdata.org/dataset/climate-change-opinion-survey)
There are a number of sheets in the excel file `climate_change_opinion_survey_aggregated_06252021.xlsx` included in the project files. They are
    
    + "climate_awareness",
    + "climate_happening",
    + "climate_beliefs",
    + "climate_worry",
    + "harm_personally",
    + "harm_future_gen",
    + "climate_importance",
    + "gov_priority", 
    + "gov_more_less",
    + "paris_support_oppose",
    + "economic_impact",
    + "renewable_more_less",
    + "fossil_more_less",
    + "climate_action",
    + "climate_info",
    + "ideology_us",
    + "ideology_row",
    + "partyid_us"

You should be able to use the format below to access each of the sheets. Furthermore, the file

    "climate_change_opinion_survey_aggregated_06252021_description.xlsx"
    
contains the description / meta-data for the data sheets. Again, the resulting dataframes are mostly *not tidy*. You may need to tidy them first.

```{r include=FALSE}
climate_change_opinion_file <- "climate_change_opinion_survey_aggregated_06252021.xlsx"
climate_happening <- read_excel(climate_change_opinion_file, sheet = "climate_happening")
climate_beliefs <- read_excel(climate_change_opinion_file, sheet = "climate_beliefs")
# etc..

climate_happening %>% head()
climate_beliefs %>% head()
# etc..

glimpse(climate_happening)
glimpse(climate_beliefs)
```
Tidy data sets
```{r}
climate_happening %>% 
  pivot_longer('Argentina':'Vietnam', names_to = "Country", values_to = "Percentage") %>% 
  pivot_wider(names_from = 'climate_happening', values_from = 'Percentage')
```
```{r}
climate_beliefs %>% 
  pivot_longer('Argentina':'Vietnam', names_to = "Country", values_to = "Percentage") %>% 
  pivot_wider(names_from = 'climate_beliefs', values_from = 'Percentage')
```

- International energy usage, including import and export, information, from [U.S. Energy Information Administration](https://www.eia.gov/international/data/world). They are contained in a number of files:

    + `INT-Export-BioFuel-09-09-2022_22-28-53.csv`
    + `INT-Export-CoalCoke-09-09-2022_23-09-12.csv`
    + `INT-Export-Electricity-09-09-2022_23-11-57.csv`
    + `INT-Export-Emissions-09-09-2022_23-18-25.csv`
    + `INT-Export-HydroCarbonLiquids-09-09-2022_23-14-09.csv`
    + `INT-Export-NaturalGas-09-09-2022_23-11-09.csv`
    + `INT-Export-PetroLiquids-09-09-2022_23-10-18.csv`
    + `INT-Export-PrimaryEnergy-09-09-2022_23-12-37.csv`

each pertaining to a particular class of energy source. You may want to write a simple function to load these files by their respective names, as they follow a common format. Again, the resulting dataframes are mostly *not tidy*. You may need to tidy them first.

```{r include=FALSE}
# Example loading the BioFuel data
BioFuel_file <- "INT-Export-BioFuel-09-09-2022_22-28-53.csv"
BioFuel_usage <- read_csv(BioFuel_file, skip = 1)
BioFuel_usage %>% head()
```
Tidying data set
```{r}
BioFuel_usage %>% 
  select(-1) %>%
  rename('Type' = 1) %>%
  pivot_longer('1980':'2019', names_to = "Year", values_to = "Usage") 
```

Import Coal Coke File
```{r}
CC_file <- "INT-Export-CoalCoke-09-09-2022_23-09-12.csv"
CoalCoke_usage <- read_csv(CC_file, skip = 1)
CoalCoke_usage %>% head()
```

Tidy dataset
```{r}
CoalCoke_usage %>% 
  pivot_longer('1949':'2021', names_to = "Year", values_to = "Usage")
```

Happiness Data Set
```{r}
happy_file <- "WorldHappinessReport2022-Score.csv"
happy <- read_csv(happy_file)
happy %>% head()
```

- [U.S. Drought severity and coverage index](https://droughtmonitor.unl.edu/DmData/DataDownload/DSCI.aspx) from U.S. Drought Monitor.
The data description is contained in the file `DSCI_fact_sheet.pdf` included in the project files, which is also available from [U.S. Drought Monitor](https://droughtmonitor.unl.edu/About/AbouttheData/DSCI.aspx). It is produced through a partnership between the National Drought
Mitigation Center at the University of Nebraska-Lincoln, the United States Depart-
ment of Agriculture, and the National Oceanic and Atmospheric Administration. The file included covers from January 01, 2000 to August 15th, 2022.
```{r include=FALSE}
Drought_population_file <- "ComprehensiveStatsByState-PopPercent-20000101_20220815.csv"
US_Drought_population <- read_csv(Drought_population_file)
US_Drought_population %>% head()
```
- U.S. Social Capital data from Meta (formerly Facebook), hosted on [HDX humdata.org](https://data.humdata.org/dataset/social-capital-atlas). The website [socialcapital.org](https://www.socialcapital.org/?dimension=EconomicConnectednessIndividual&dim1=EconomicConnectednessIndividual&dim2=CohesivenessClustering&dim3=CivicEngagementVolunteeringRates&geoLevel=county&selectedId=06037) has further discussion about this data. Among the project files, the license and other information for this data set is contained in `license.pdf`, `data_release_readme_31_07_2022_nomatrix.pdf` and `socialcapital_nontech.pdf`.
The data set included contains information by county. You may want to clean up the column`county_name` by splitting it into two, containing the state and county separately.

```{r include=FALSE}
social_capital_file <- "social_capital_county.csv"
social_capital <- read_csv(social_capital_file)
social_capital %>% head()
```

### Real-time COVID-19 data sets:

- `covid_complete`: [COVID-19 cases (Our world in data)](https://covid.ourworldindata.org/data/owid-covid-data.csv)
```{r results='hide', warning=FALSE}
# need to remove include=FALSE above when send out as project file
covid_address <- "https://covid.ourworldindata.org/data/owid-covid-data.csv"
# may also try  "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"
covid_cases <- read_csv(covid_address,
                        col_types = cols(
                          .default = col_double(),
                          date = col_date(format =""),
                          iso_code = col_character(),
                          location = col_character(),
                          continent = col_character(),
                          tests_units = col_character()
                        )
)
covid_cases %>% head()
```

Use the above data for your project. You do not need to look for extra data sets for this project. You may look at other data sets for ideas and inspirations, but in the analysis and report, only use the data sets provided above.

============================
Sanity check
============================

